<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="shortcut icon" href="./Jianguo Zhang_files/myIcon.ico">

<meta name="keywords" content="Jianguo Zhang, CS, SE, ZJU, Zhejiang University"> 
<meta name="description" content="Jianguo Zhang&#39;s Homepage">
<link rel="stylesheet" href="./Jianguo Zhang_files/jemdoc.css" type="text/css">
<title>Jianguo Zhang</title>
<script type="text/javascript" async="" src="./Jianguo Zhang_files/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99347283-1', 'auto');
  ga('send', 'pageview');

</script>
<script type="text/javascript" src="./Jianguo Zhang_files/jquery-1.12.4.min.js"></script></head>
<body>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Jianguo Zhang 张建国</h1><h1>
				</h1></div>

				<h3>Ph.D in Computer Science</h3>
				<p>
					University of Illinois at Chicago (UIC),<br>
					Chicago, IL, U.S. <br>
					<br>
					Email: jianguozhang@salesforce.com 
					(<u>Note:</u> jzhan51@uic.edu is deactivated by UIC)
				</p>
				<p>
					<a href="https://github.com/jianguoz" target="_blank"><img src="./Jianguo Zhang_files/items/Github.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://scholar.google.com/citations?user=mAAVFEsAAAAJ&hl=en&oi=ao" target="_blank"><img src="./Jianguo Zhang_files/items/google_scholar.png" height="30px" style="margin-bottom:-3px"></a> 
 					<a href="https://twitter.com/JianguoZhang3" target="_blank"><img src="./Jianguo Zhang_files/items/Twitter_logo2013.png" height="30px" style="margin-bottom:-3px"></a> 	
				</p>
			</td>
			<td>
				<img src="./Jianguo Zhang_files/photos/personal_latest.jpg" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--  <h2>Biography <!--[<a href="./Jianguo Zhang_files/files/Jianguo_Zhang.pdf" target="_blank">Resume</a>]--></h2>

I am a research scientist at <a href="https://www.salesforceairesearch.com/">Salesforce AI Research</a>. I received Ph.D. in the Department of Computer Science at the University of Illinois at Chicago. During my doctoral study, I was advised by Prof. <a href="https://scholar.google.com/citations?user=D0lL1r0AAAAJ&hl=en">Philip S. Yu</a> in the <a href="https://bdsc-uic.github.io/index.html">BDSC Lab</a>. 
<!-- Currently I am a research intern at <a href="https://einstein.ai/">Salesforce Research</a>, working with <a href="http://cmxiong.com/
	">Caiming Xiong</a>,  <a href="http://www.logos.t.u-tokyo.ac.jp/~hassy/
	">Kazuma Hashimoto</a> and  <a href="https://jasonwu0731.github.io/">Chien-Sheng Wu</a>.  -->
<!-- in the <a href="https://bdsc.lab.uic.edu/">BDSC Lab</a>, -->
<!-- 	<br><br> -->
<!-- My previous research (before March 2019) focuses on adversarial training and its applications, and currently, I am mainly studying Conversational AI. My mission is to create accurate and smart dialogue systems. My current research focuses on: -->
	My current research mainly forcuses on Conversational AI and AI Agent. Specifically, 
	<li> AI agents, e.g., AI agent design, advanced data collection and language model training.</li>
	<li> Natural language understanding, e.g., few-shot natural language understanding/classification/generation</li>
	<li> End-to-end dialogue system, e.g., accurate dialogue state tracking and dialogue systems without complicated pipelines</li>
<!-- <li> Natural language understanding, e.g., robust and efficient few-shot intent detection and out-of-scope prediction</li> -->
</li>

<br><br>
	<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Rancho&effect=ice|fire-animation|neon|splintered|grass">

<h2>News</h2>  
<!-- 	<li><b> May 2022</b>: ❤️ Welcome to maintain <a href="https://github.com/jianguoz/BiWeekly-Research-Paper-Series">(Bi) Weekly-Research-Paper-Series</a> together!  -->
	<li><b>Jan 2022</b>: I will join Salesforce AI Research as a Research Scientist.
	</li> 
	<li><b>Sep 2021</b>: Finished my internship at <a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a>. The project focuses on few-shot end-to-end task-oriented dialgue systems.  
	</li> 
	<li><b>Aug 2021</b>: Two paper are accepted by <font color=#527bbd><a href="https://2021.emnlp.org/">EMNLP 2021</a></font> main conference as short papers. One first-author work collaborates with Adobe Research and it focuses on natural language understanding. 
	</li> 
	<li><b>June 2021</b>: Check our new released <font color=#527bbd><a href="https://arxiv.org/abs/2106.04564">paper</a></font>  and public resources for <font color=#527bbd><a href="https://github.com/jianguoz/Few-Shot-Intent-Detection">few-shot intent detection</a></font>  w/ and w/o out-of-scope queries. 
<!-- 		I will join <font color=#527bbd><a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a></font> as a research intern.  -->
	</li> 
	<li><b>May 2021</b>: I finished my internship at <font color=#527bbd><a href="https://research.adobe.com/">Adobe Research</a></font> and submitted one research paper, where I also discover PhotoShop user intents and improve intent detection performance using large-scale unlabeled PhotoShop user queries. 
	</li> 
	<li><b>Dec 2020</b>: Our toolkit <font color=#527bbd><a href="https://github.com/CGCL-codes/naturalcc">NaturalCC</a></font> has been released in GitHub, which can be accessed via the <font color=#527bbd><a href="https://xcodemind.github.io/">Homepage</a></font>.
	</li>
	<li><b>Nov 2020</b>: We have released the <font color=#527bbd><a href="https://arxiv.org/abs/2010.13009">paper</a></font>, <font color=#527bbd><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference">video</a></font> for the EMNLP 2020 paper, and the corresponding code is available <font color=#527bbd><a href="https://github.com/salesforce/DNNC-few-shot-intent">here</a></font>!
	</li>
	<li><b>Sep 2020</b>: One first-author work collaborated with Salesforce Research is accepted by <font color=#527bbd><a href="https://sites.google.com/view/starsem2020/">*SEM 2020</a></font>  as a long paper. The work focuses on multi-domain dialog state tracking and it originally got borderline scores on ACL 2020. We have updated the <font color=#527bbd><a href="https://arxiv.org/pdf/1910.03544.pdf">paper</a></font>!
	</li>
	<li><b>Sep 2020</b>: One first-author work collaborated with Salesforce Research is accepted by <font color=#527bbd><a href="https://2020.emnlp.org/">EMNLP 2020</a></font> main conference as a long paper. The work focuses on natural language understanding. 
	</li>
	<li><b>June 2020</b>: One work collaborated with Google Research is accepted by <font color=#527bbd><a href="https://sites.google.com/view/2ndnlp4convai/">ACL 2020 NLP4ConvAI</a></font>, and the modified high-quality  version of <font color=#527bbd><a href="https://www.aclweb.org/anthology/2020.nlp4convai-1.13.pdf">MultiWOZ 2.2 dataset</a></font> with additional annotation
corrections and state tracking baselines is released  <font color=#527bbd><a href="https://github.com/budzianowski/multiwoz">here</a></font>!
	</li>
	
<!-- <h2>Preprints</h2>
<!-- <li>
	<b>Few-shot End-to-End Task-Oriented Dialogue Systems with Database</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Zhang et al.</b> 
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [Paper] -->
<!-- </li>
	<li>
	<b>Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Yao Wan, Ye Liu, Caiming Xiong, Philip Yu</font>
<!-- 	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://2021.eacl.org/">EACL 2021</a></font>  -->
	<!-- <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2106.04564">[Paper]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/jianguoz/Few-Shot-Intent-Detection" >Resources</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> 
</li>  -->

<h2>Open Sources</h2> 
<!-- 	<li> -->
    <p><a href="https://github.com/salesforce/DialogStudio">DialogStudio</a> | <a href="https://github.com/budzianowski/multiwoz">MultiWOZ</a> | <a href="https://github.com/CGCL-codes/naturalcc">NaturalCC</a> </p>
<!-- </li> -->

<h2>Publications</h2>
<!-- <p><b>Last Update: March 2022. See more on <a href="https://scholar.google.com">Google Scholar</a> or <a href="https://scholar.google.com">Semantic Scholar</a></b></p> -->

<ul>

<li>
	<b>DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yu Wang, Zhiwei Liu,  <b>Jianguo Zhang</b><font color=#808080 >, Weiran Yao, Shelby Heinecke, Philip S. Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2312.11336.pdf">[ArXiv Paper]</a></font> 
</li>	

<li>
	<b>Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yao Wan, Yang He, Zhangqian Bi, <b>Jianguo Zhang</b><font color=#808080 >, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin, Philip S. Yu.</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2401.00288.pdf">[ArXiv Paper]</a></font> / <font color=#527bbd><a href="https://xcodemind.github.io/">[Sources]</a></font> 
</li>	

<li>
	<b>BOLAA: Benchmarking and orchestrating LLM-augmented autonomous agents</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Zhiwei Liu, Weiran Yao, <b>Jianguo Zhang</b><font color=#808080 >, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2308.05960.pdf">[ArXiv Paper]</a></font> / <font color=#527bbd><a href="https://github.com/salesforce/BOLAA">[GitHub]</a></font> 
</li>	
	
<li>
	<b>DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, Caiming Xiong</font>
<!-- 	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2023.sigdial.org/">SIGDIAL 2023</a></font>  -->
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2307.10172.pdf">[ArXiv Paper]</a></font> / <font color=#527bbd><a href="https://github.com/salesforce/DialogStudio">[GitHub]</a></font> / <font color=#527bbd><a href="https://huggingface.co/datasets/Salesforce/dialogstudio">[HuggingFace]</a></font> 
</li>	

<li>
	<b>Retroformer: Retrospective large language agents with policy gradient optimization</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, <b>Jianguo Zhang</b><font color=#808080 >, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2308.02151.pdf">[ArXiv Paper]</a></font> / <font color=#527bbd><a href="https://github.com/salesforce/BOLAA">[GitHub]</a></font> 
</li>	

<li>
	<b>Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Stephen Roller, Kun Qian, Zhiwei Liu, Rui Meng, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2023.sigdial.org/">SIGDIAL 2023</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2023.sigdial-1.47/">[Paper]</a></font> 
</li>	

<li>
	<b>Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-Oriented Dialogue Systems</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Yihao Feng, Shentao Yang, Shujian Zhang, <b>Jianguo Zhang</b><font color=#808080 >, Caiming Xiong, Mingyuan Zhou, Huan Wang</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href=https://iclr.cc/Conferences/2023/">ICLR 2023</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2302.10342.pdf">[Paper]</a></font> 
</li>	
	
<li>
	<b>Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ziwei Fan, Zhiwei Liu, Shelby Heinecke, <b>Jianguo Zhang</b><font color=#808080 >, Huan Wang, Caiming Xiong, Philip S Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://uobevents.eventsair.com/cikm2023/">CIKM 2023</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://dl.acm.org/doi/10.1145/3583780.3615110/">[Paper]</a></font> 
</li>	
	
<li>
	<b>Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Yao Wan, Zhiwei Liu, Ye Liu, Caiming Xiong, Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://sites.google.com/view/4thnlp4convai/home">ACL 2022</a> Workshop on NLP for Conversational AI</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2106.04564.pdf">[Paper]</a></font> <font color=#527bbd><a href="https://github.com/jianguoz/Few-Shot-Intent-Detection">[Resources]</a></font> 
</li>	
		
<li>
	<b>NaturalCC: An Open-Source Toolkit for Code Intelligence</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Yao Wan, Yang He, Zhangqian Bi, </font> <b>Jianguo Zhang</b><font color=#808080 >, Yulei Sui, Hongyu Zhang, Kazuma Hashimoto, Hai Jin, Guandong Xu, Caiming Xiong and Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of<a href="https://conf.researchr.org/track/icse-2022/icse-2022-demo---demonstrations"> ICSE 2022 Demo Track</a></font>  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/CGCL-codes/naturalcc" >Toolkit</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> 
</li>

<li>
	<b>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang and Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2021.emnlp.org/">EMNLP 2021</a></font> (Oral)</font>
<!-- 	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2021.emnlp.org/">EMNLP 2021</a></font> short paper <font color='red'>(</font><font color='red'>Oral</font><font color='red'>)</font> -->
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2021.emnlp-main.144.pdf">[Paper]</a></font> 
<!-- 	/ <font color=#527bbd><a href="https://drive.google.com/file/d/19l6XQ6YO6rNILPkTw890qrTHfOTEYv3o/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference" >Video</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/salesforce/DNNC-few-shot-intent" >Code</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font>  -->
</li>

<li>
	<b>HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ye Liu, </font> <b>Jianguo Zhang</b><font color=#808080 >, Yao Wan, Congying Xia, Lifang He and Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2021.emnlp.org/">EMNLP 2021</a></font> (Oral)</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://aclanthology.org/2021.emnlp-main.13.pdf">[Paper]</a></font> 
<!-- 	/ <font color=#527bbd><a href="https://drive.google.com/file/d/19l6XQ6YO6rNILPkTw890qrTHfOTEYv3o/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference" >Video</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/salesforce/DNNC-few-shot-intent" >Code</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font>  -->
</li>
	
<li>
	<b>Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Ye Liu, Yao Wan, </font> <b>Jianguo Zhang</b><font color=#808080 >, Wenting Zhao, Philip Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2021.eacl.org/">EACL 2021</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.aclweb.org/anthology/2021.eacl-main.105/">[Paper]</a></font> 
<!-- 	/ <font color=#527bbd><a href="https://drive.google.com/file/d/19l6XQ6YO6rNILPkTw890qrTHfOTEYv3o/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font color='red'><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference" style="color:red">Video</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/salesforce/DNNC-few-shot-intent" >Code</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font>  -->
</li>
	
<li>
	<b>Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan, Philip S Yu, Richard Socher, Caiming Xiong</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://2020.emnlp.org/">EMNLP 2020</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/abs/2010.13009">[Paper]</a></font> / <font color=#527bbd><a href="https://drive.google.com/file/d/19l6XQ6YO6rNILPkTw890qrTHfOTEYv3o/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://slideslive.com/38939288/discriminative-nearest-neighbor-fewshot-intent-detection-by-transferring-natural-language-inference" >Video</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> / <font color=#527bbd><a href="xxxxx">[</a></font><font ><a href="https://github.com/salesforce/DNNC-few-shot-intent" >Code</a></font><font color=#527bbd></font><font color=#527bbd><a href="xxxxx">]</a></font> 
</li>
<li>
	<b>Find or Classify? Dual Strategy for Slot-Value Predictions on Multi-Domain Dialog State Tracking</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip S Yu, Richard Socher, Caiming Xiong </font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://sites.google.com/view/starsem2020/">*SEM 2020</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.aclweb.org/anthology/2020.starsem-1.17/">[Paper]</a></font> / <font color=#527bbd><a href="https://drive.google.com/file/d/1iBuZT13A3_SAtKqlP6OGpzi4FdzQRGRl/view?usp=sharing">[Slide]</a></font> / <font color=#527bbd><a href="xxxxx">[Video]</a></font>  <font >(</font><font>Oral</font><font >)</font>
</li>
<li>
	<b>MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta,</font> <b>Jianguo Zhang</b><font color=#808080 >, Jindong Chen</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://sites.google.com/view/2ndnlp4convai/">ACL 2020</a> Workshop on NLP for Conversational AI</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/2007.12720.pdf">[Paper]</a></font> / <a href="xxxxx">[</a></font><font ><a href="https://github.com/budzianowski/multiwoz" >Dataset</a> <font color=#527bbd><font color=#527bbd><a href="xxxxx">]</a><a href="https://slideslive.com/38929641/multiwoz-22-a-dialogue-dataset-with-additional-annotation-corrections-and-state-tracking-baselines">[Video]</a></font> </font><font color=#527bbd></font></font> 
</li>
<li>
	<b>Multi-Modal Generative Adversarial Network for Short Product Title Generation in Mobile E-Commerce</b> &nbsp; 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Pengcheng Zou, Zhao Li, Yao Wan, Xiuming Pan, Yu Gong, Philip S Yu</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://naacl.org/naacl-hlt-2019/">NAACL-HLT 2019</a></font> (Oral) </font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://www.aclweb.org/anthology/N19-2009/">[Paper]</a></font>  
<!-- 		<font color='red'>(</font><font color='red'>Oral</font><font color='red'>)</font> -->
<!-- 		<font color='red'>(</font><font color='red'>Oral Presentation</font><font color='red'>)</font> -->
	
<!--       <button style="background-color:red"; title="Natural Language Processing">NLP</button> -->
</li>
<li>
	<b>Product Title Refinement via Multi-Modal Generative Adversarial Learning</b> &nbsp;  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang</b><font color=#808080 >, Pengcheng Zou, Zhao Li, Yao Wan, Ye Liu, Xiuming Pan, Yu Gong, Philip S Yu</font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://nips.cc/Conferences/2018/">NeurIPS 2018</a> Workshop on ViGIL</font>
<!-- 		(A short arXiv version of the NAACL-HLT 2019 paper and no Proceedings for the Workshop) -->
</li>
<li>
	<b>Layerwise Perturbation-Based Adversarial Training for Hard Drive Health Degree Prediction</b> &nbsp;  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>Jianguo Zhang*</b><font color=#808080 >, Ji Wang*, Lifang He, Zhao Li, Philip S Yu (* indicates equal contribution)</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="http://icdm2018.org/">ICDM 2018</a></font> 
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/1809.04188.pdf">[Paper]</a></font>
</li>
<li>
	<b>Not just privacy: Improving performance of private deep learning in mobile cloud</b> &nbsp;  
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#808080 >Ji Wang*,</font> <b>Jianguo Zhang*</b> <font color=#808080 >, Weidong Bao, Xiaomin Zhu, Bokai Cao, Philip S Yu (* indicates equal contribution)</font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd>In Proceedings of <a href="https://www.kdd.org/kdd2018/">KDD 2018</a></font>
	<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <font color=#527bbd><a href="https://arxiv.org/pdf/1809.03428.pdf">[Paper]</a></font>
</li>

</ul>




 <br><br>

<div id="experience">
<h2>Experience</h2>

<table id="tbPublications" width="100%"> 
<tr>
        <td>
            <p><a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a>, New York, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp June 2021 - Sep. 2021</p> 
<!--  	<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019, &nbsp Summer 2019</p> --> 
          <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentor:  <a href="https://stephenroller.com/
	">Stephen Roller</a>
<!--                  ,<a href="https://david-yoon.github.io/">David Seunghyun Yoon</a>, <a href="https://research.adobe.com/person/xiang-chen/">Xiang Chen</a> and <a href="https://research.adobe.com/person/walter-chang/">Walter Chang</a>. -->
            </p>  
<!-- 	     <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp (I become an NLPer from here!) -->
        </td> 
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/fair-3.png" height=58px></p>
        </td>
  </tr> 

	<tr>
        <td>
            <p><a href="https://research.adobe.com/">Adobe Research</a>, San Jose, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Mar. 2021 - May 2021</p> 
<!--  	<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019, &nbsp Summer 2019</p> --> 
          <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Collaborators:  <a href="https://sites.google.com/site/trungbuistanford/Home
	">Trung Bui</a>,
                 <a href="https://david-yoon.github.io/">David Seunghyun Yoon</a> and <a href="https://research.adobe.com/person/walter-chang/">Walter Chang</a>.
            </p> 
<!-- 	     <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp (I become an NLPer from here!) -->
        </td> 
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/Adobe-Logo.png" height=58px></p>
        </td>
    </tr> 

	
<tr>
        <td>
            <p><a href="https://einstein.ai/">Salesforce Research</a>, Palo Alto, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019 - Summer 2020</p> 
<!--  	<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Spring 2019, &nbsp Summer 2019</p> --> 
             <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors:  <a href="http://www.logos.t.u-tokyo.ac.jp/~hassy/
	">Kazuma Hashimoto</a>,
                 <a href="http://cmxiong.com/">Caiming Xiong</a>, <a href="https://jasonwu0731.github.io/">Chien-Sheng Wu</a> and <a href="https://www.socher.org/">Richard Socher</a>.
            </p> 
	     <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp (I become an NLPer from here!)
        </td>
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/salesforce-research.png" height=80px></p>
        </td>
    </tr> 

    <tr>
        <td>
            <p><a href="https://damo.alibaba.com/">Alibaba Group</a>, Hangzhou, China</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Summer 2018</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors: 
                <a href="https://www.linkedin.com/in/pengcheng-zou-765640105/?originalSubdomain=cn">Pengcheng Zou</a> and <a href="https://sites.google.com/view/zhaoli">Zhao Li</a>.
            </p> 
        </td>
        <td align="center">
            <p><img src="./Jianguo Zhang_files/photos/alibaba-group.png" height=76px></p> 
        </td>
    </tr>
</table>
</div>

<h2>Service</h2> 
<li><b>Program Committee/Reviewer</b>: ACL Rolling Review 2021-pres, EMNLP 2021-Pres, NeurIPS 2023, EACL 2023, ACL 2021/2022, NAACL 2021, IEEE/ACM TASLP 2020-Pres, and others. </li>
<li><b>Session Chair</b>: COLING 2020 (industry track: Dialogue).</li>
<li><b>Senior Program Committee</b>: CIKM 2023.</li>
<br><br> 
	
<!-- <h2>Misc</h2> 
I am a <a href="https://github.com/jianguoz/Conversational-AI/blob/master/misc/images_canon/readme.md"> photographer </a> (Keep learning), guitar lover, pet lover (My cat name is <a href="https://github.com/jianguoz/Conversational-AI/blob/master/misc/Maomao/readme.md">Maomao</a>), Chinese literature (Already read and forget many books) and basketball (Go Lakers, Metamind Lebron) fan.  
<!-- <a href="https://info.flagcounter.com/BimD"><img src="https://s04.flagcounter.com/mini/BimD/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a> <a href="https://github.com/jianguoz/Conversational-AI/blob/master/misc/images_canon/readme.md"> -->
<!-- <br><br>   -->

<h2>This website is under construction</h2>

<!--  <a href="https://info.flagcounter.com/BimD"><img src="https://s04.flagcounter.com/mini/BimD/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>  -->
</div>
</body></html>
